{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always reload modules to have the current version\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranking.normalization import normalizer as n\n",
    "from ranking.util import dataset_paths as dp\n",
    "from ranking.util import json_lines as jl\n",
    "from tqdm import tqdm\n",
    "from tree_sitter import Language, Parser\n",
    "import pandas as pd\n",
    "\n",
    "input_file = dp.raw_corpus\n",
    "cleaned_unique_functions_output  = 'parsed-complete-all-unique-functions.jsonl'\n",
    "tokenized_output = 'tok-' + cleaned_unique_functions_output\n",
    "lemmatized_output = 'lem-' + cleaned_unique_functions_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language.build_library(\n",
    "  # Store the library in the `build` directory\n",
    "  'tree-sitter/build/my-languages.so',\n",
    "\n",
    "  # Include one or more languages\n",
    "  [\n",
    "    'tree-sitter\\\\bindings\\\\tree-sitter-haskell'\n",
    "  ]\n",
    ")\n",
    "\n",
    "HS_LANGUAGE = Language('tree-sitter\\\\build\\\\my-languages.so', 'haskell')\n",
    "parser = Parser()\n",
    "parser.set_language(HS_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_sig(text: str) -> bool:\n",
    "    tree = parser.parse(bytes(text, \"utf8\"))\n",
    "    return not tree.root_node.has_error and tree.root_node.children[0].type == 'signature'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_type(sig: str, query) -> str:\n",
    "    tree = parser.parse(bytes(sig, \"utf8\"))\n",
    "    assert(tree.root_node.children[0].type == 'signature')\n",
    "    captures = query.captures(tree.root_node)\n",
    "    return str(captures[0][0].text, 'UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_extra_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def equalize_docItem(docItem):\n",
    "    no_empty_ctx = \"\".join(docItem.split('() =>'))\n",
    "    return strip_extra_spaces(no_empty_ctx)\n",
    "\n",
    "sign = '($$!) :: () => (i -> r) -> Number r i -> r'\n",
    "exptectedSign = '($$!) :: (i -> r) -> Number r i -> r'\n",
    "\n",
    "assert(equalize_docItem(sign) == exptectedSign)\n",
    "assert(exptectedSign == equalize_docItem(exptectedSign))\n",
    "\n",
    "sign = 'hoistDiT :: () => (forall x . () => n x -> m x) -> (forall x . () => m x -> n x) -> DiT level path msg m a -> DiT level path msg n a'\n",
    "exptectedSign = 'hoistDiT :: (forall x . n x -> m x) -> (forall x . m x -> n x) -> DiT level path msg m a -> DiT level path msg n a'\n",
    "\n",
    "assert(equalize_docItem(sign) == exptectedSign)\n",
    "assert(exptectedSign == equalize_docItem(exptectedSign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a storageId that acts a group id to each function\n",
    "def group_unique_functions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['equalizedDocItem'] = df.apply(\n",
    "        lambda row: equalize_docItem(row['docItem']), axis=1)\n",
    "    df['docContentLen'] = df['docContent'].str.len()\n",
    "    df.sort_values('docContentLen', ascending=False, inplace=True) # sort to have the longest documentation at the top position in each group\n",
    "    groups = df.groupby(['equalizedDocItem'])\n",
    "\n",
    "    df['storageId'] = groups.ngroup()\n",
    "    df['docContent'] = groups['docContent'].transform('first')\n",
    "    df['docItem'] = groups['docItem'].transform('first')\n",
    "    df['docType'] = groups['docType'].transform('first')\n",
    "    return df[['docId', 'storageId', 'docContent', 'docItem', 'docType', 'docPackage']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    groups = df.groupby('storageId')\n",
    "    tqdm.pandas(desc='Lemmatizing + Stop Word Removal')\n",
    "    df['docContent'] = groups['docContent'].transform('first').progress_apply(lambda content: n.normalize(content, stop=n.get_wn_stopwords()))\n",
    "    return df\n",
    "\n",
    "def only_tokenize_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    groups = df.groupby('storageId')\n",
    "    tqdm.pandas(desc='Tokenization Only')\n",
    "    df['docContent'] = groups['docContent'].transform('first').progress_apply(lambda content: n.normalize(content, lambda text: text))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|██████████| 805414/805414 [00:21<00:00, 36704.80it/s]\n",
      "Extracting signatures: 100%|██████████| 805414/805414 [00:31<00:00, 25519.70it/s]\n",
      "Extracting types of signatures: 100%|██████████| 470571/470571 [00:29<00:00, 16214.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df = jl.read_jsonl(input_file)\n",
    "tqdm.pandas(desc='Cleaning')\n",
    "df['docContent'] = df['docContent'].progress_apply(n.clean)\n",
    "\n",
    "tqdm.pandas(desc='Extracting signatures')\n",
    "df['is_signature'] = df['docItem'].progress_apply(contains_sig)\n",
    "df = df[df['is_signature'] == True] # ignore all items that are no signatures or contain parsing errors\n",
    "\n",
    "tqdm.pandas(desc='Extracting types of signatures')\n",
    "query = HS_LANGUAGE.query('(signature _ type: _ _ @type)')\n",
    "df['docType'] = df['docItem'].progress_apply(lambda item: f':: {get_sig_type(item, query)}')\n",
    "df = group_unique_functions(df).sort_values('storageId')\n",
    "jl.to_jsonl(df, cleaned_unique_functions_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok = only_tokenize_dataset(df.copy())\n",
    "df_lem = normalize_dataset(df.copy())\n",
    "\n",
    "jl.to_jsonl(df_tok, tokenized_output)\n",
    "jl.to_jsonl(df_lem, lemmatized_output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b39dcb3d3362ff747ddce896d3961166e3de0f67646b9052166a268c283e04ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
