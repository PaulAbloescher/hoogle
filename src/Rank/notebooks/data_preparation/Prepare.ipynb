{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from os import path\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text, language='english'):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop = set(stopwords.words(language))\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = lemmatize(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, language='english'):\n",
    "    stop = set(stopwords.words(language))\n",
    "    return [token for token in word_tokenize(text) if token not in stop and token.isalnum()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens, language='english'):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    corpus = {}\n",
    "    with open(file) as fin:\n",
    "        for row in fin:\n",
    "            jsonLine = json.loads(row)\n",
    "            corpus[jsonLine['id']] = jsonLine['content']\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_lines(file):\n",
    "    corpus = []\n",
    "    with open(file, encoding='utf-8') as fin:\n",
    "        for row in fin:\n",
    "            corpus.append(row.rstrip('\\n'))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_ids(file):\n",
    "    ids = []\n",
    "    with open(file, encoding='utf-8') as fin:\n",
    "        for row in fin:\n",
    "            ids.append(row.rstrip('\\n'))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFile(targetFile, language='english'):\n",
    "    targetFileName = path.basename(targetFile)\n",
    "    destinationFileName = \"tokenized.\" + targetFileName\n",
    "    with open(targetFile) as fin, open(destinationFileName, 'w', newline='') as fout:\n",
    "        for row in fin:\n",
    "            # jsonLine = json.loads(row)\n",
    "            # content = jsonLine['content']\n",
    "            # jsonLine['content'] = \" \".join(normalize(content))\n",
    "            jsonLine = normalize(row)\n",
    "            print(\" \".join(jsonLine), file=fout)\n",
    "            # print(json.dumps(jsonLine), file=fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizeFile('.\\\\plain.raw.txt')\n",
    "\n",
    "# with open ('dump.tokenized.txt') as fin:\n",
    "#     lines = fin.read()\n",
    "#     count = Counter(lines.split())\n",
    "#     print(len(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = read_corpus_lines('tokenized.plain.raw.txt')\n",
    "corpus_tokenized = [document.split(\" \") for document in corpus]\n",
    "bm25 = BM25Okapi(corpus_tokenized)\n",
    "# corpus = read_corpus('tokenized.raw.dump.jsonl')\n",
    "# corpusDocsOnly = [doc.split(\" \") for doc in list(corpus.values())]\n",
    "# bm25 = BM25Okapi(corpusDocsOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['encode',\n",
       "  'string',\n",
       "  'base64',\n",
       "  'form',\n",
       "  'result',\n",
       "  'always',\n",
       "  'multiple',\n",
       "  '4',\n",
       "  'byte',\n",
       "  'length'],\n",
       " ['encode',\n",
       "  'string',\n",
       "  'base64',\n",
       "  'form',\n",
       "  'result',\n",
       "  'always',\n",
       "  'multiple',\n",
       "  '4',\n",
       "  'byte',\n",
       "  'length'],\n",
       " ['encode',\n",
       "  'string',\n",
       "  'base64',\n",
       "  'form',\n",
       "  'result',\n",
       "  'always',\n",
       "  'multiple',\n",
       "  '4',\n",
       "  'byte',\n",
       "  'length'],\n",
       " ['serialize', 'base64', 'encode', 'secret', 'key'],\n",
       " ['encode', 'single', 'part', 'use', 'base64', 'binary', 'data'],\n",
       " ['encrypt',\n",
       "  'authenticate',\n",
       "  'encode',\n",
       "  'base64',\n",
       "  'given',\n",
       "  'cookie',\n",
       "  'data',\n",
       "  'returned',\n",
       "  'byte',\n",
       "  'string',\n",
       "  'ready',\n",
       "  'used',\n",
       "  'response',\n",
       "  'header'],\n",
       " ['get',\n",
       "  'base64',\n",
       "  'encode',\n",
       "  'bytestring',\n",
       "  'getencodedbytestring64',\n",
       "  'foobar',\n",
       "  'zm9vymfy',\n",
       "  'getencodedbytestring64'],\n",
       " ['get',\n",
       "  'base64',\n",
       "  'encode',\n",
       "  'bytestring',\n",
       "  'getencodedbytestring64',\n",
       "  'foobar',\n",
       "  'zm9vymfy',\n",
       "  'getencodedbytestring64'],\n",
       " ['get',\n",
       "  'base64',\n",
       "  'encode',\n",
       "  'bytestring',\n",
       "  'getencodedbytestring64',\n",
       "  'foobar',\n",
       "  'zm9vymfy',\n",
       "  'getencodedbytestring64'],\n",
       " ['get',\n",
       "  'base64',\n",
       "  'encode',\n",
       "  'bytestring',\n",
       "  'getencodedbytestring64',\n",
       "  'foobar',\n",
       "  'zm9vymfy',\n",
       "  'getencodedbytestring64']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'encode a string into base64'\n",
    "tQuery = normalize(query)\n",
    "bm25.get_top_n(tQuery, corpus_tokenized, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpusDocsOnly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Paul\\Repos\\hoogle-fork\\src\\Corpus\\Prepare.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Corpus/Prepare.ipynb#ch0000012?line=0'>1</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mangle degree radian simple little library dealing geometric angl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Corpus/Prepare.ipynb#ch0000012?line=1'>2</a>\u001b[0m tQuery \u001b[39m=\u001b[39m normalize(query)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Corpus/Prepare.ipynb#ch0000012?line=3'>4</a>\u001b[0m bm25\u001b[39m.\u001b[39mget_top_n(query, corpusDocsOnly)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpusDocsOnly' is not defined"
     ]
    }
   ],
   "source": [
    "query = 'angle degree radian simple little library dealing geometric angl'\n",
    "tQuery = normalize(query)\n",
    "\n",
    "bm25.get_top_n(query, corpusDocsOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_indices(bm25, query, n=5):\n",
    "    scores = bm25.get_scores(query)\n",
    "    return np.argpartition(scores, -n)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'storage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Paul\\Repos\\hoogle-fork\\src\\Rank\\Corpus\\Prepare.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Rank/Corpus/Prepare.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstorage\u001b[39;00m \u001b[39mimport\u001b[39;00m document_store \u001b[39mas\u001b[39;00m ds\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Rank/Corpus/Prepare.ipynb#ch0000014?line=2'>3</a>\u001b[0m store \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mDocumentStore()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Paul/Repos/hoogle-fork/src/Rank/Corpus/Prepare.ipynb#ch0000014?line=3'>4</a>\u001b[0m store\u001b[39m.\u001b[39mloadFromJsonl(\u001b[39m'\u001b[39m\u001b[39mtokenized.small.dump.jsonl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'storage'"
     ]
    }
   ],
   "source": [
    "import storage.document_store as ds\n",
    "\n",
    "store = ds.DocumentStore()\n",
    "store.loadFromJsonl('tokenized.small.dump.jsonl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b39dcb3d3362ff747ddce896d3961166e3de0f67646b9052166a268c283e04ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
