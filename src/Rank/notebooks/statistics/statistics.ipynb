{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always reload modules to have the current version\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranking.util import json_lines as jl\n",
    "from ranking.util import dataset_paths as dp\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = jl.read_jsonl(dp.raw_corpus)\n",
    "raw_df = raw_df[raw_df['docItem'] != '']  # ignore all items that are no functions\n",
    "unique_types_group = raw_df.groupby('docType')\n",
    "unique_types_count = unique_types_group.ngroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144897 215654\n"
     ]
    }
   ],
   "source": [
    "unique_functions = jl.read_jsonl(dp.unique_functions_corpus)\n",
    "unique_functions_group = unique_functions.groupby('storageId')\n",
    "unique_functions_count = unique_functions_group.ngroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tokenized     lemmatized\n",
      "count  215654.000000  215654.000000\n",
      "mean       19.350742      11.123601\n",
      "std        38.030297      21.870450\n",
      "min         0.000000       0.000000\n",
      "25%         6.000000       4.000000\n",
      "50%        11.000000       6.000000\n",
      "75%        20.000000      12.000000\n",
      "max      3160.000000    1936.000000\n",
      "total tokenized words: 4173065\n",
      "total lemmatized words: 2398849\n"
     ]
    }
   ],
   "source": [
    "tok_unique_functions = jl.read_jsonl(dp.tokenized_unique_functions_corpus)\n",
    "tok_unique_functions = tok_unique_functions.groupby('storageId').first()\n",
    "\n",
    "lem_unique_functions = jl.read_jsonl(dp.lemmatized_unique_functions_corpus)\n",
    "lem_unique_functions = lem_unique_functions.groupby('storageId').first()\n",
    "\n",
    "tok_unique_functions_doc_len = tok_unique_functions['docContent'].str.split().str.len()\n",
    "lem_unique_functions_doc_len = lem_unique_functions['docContent'].str.split().str.len()\n",
    "\n",
    "# Total\n",
    "tok_unique_functions_doc_len_sum = tok_unique_functions_doc_len.sum()\n",
    "lem_unique_functions_doc_len_sum = lem_unique_functions_doc_len.sum()\n",
    "\n",
    "unique_functions_stat = pd.concat([tok_unique_functions_doc_len.describe().rename('tokenized'), lem_unique_functions_doc_len.describe().rename('lemmatized')], axis=1)\n",
    "print(unique_functions_stat)\n",
    "print('total tokenized words:', tok_unique_functions_doc_len_sum)\n",
    "print('total lemmatized words:', lem_unique_functions_doc_len_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words tokenized 114803\n",
      "unique words lemmatized 110939\n",
      "total tokenized words: 5616723\n",
      "total lemmatized words: 3259828\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tok_fastText_corpus = pd.read_csv(dp.tokenized_unique_sentences_corpus, header=None, converters={0: str}).squeeze('columns')\n",
    "lem_fastText_corpus = pd.read_csv(dp.lemmatized_unique_sentences_corpus, header=None, converters={0: str}).squeeze('columns')\n",
    "total_tok_corpus = tok_fastText_corpus.str.split().str.len().sum()\n",
    "total_lem_corpus = lem_fastText_corpus.str.split().str.len().sum()\n",
    "\n",
    "unique_words_tok_res = Counter()\n",
    "unique_words_lem_res = Counter()\n",
    "unique_words_tok = tok_fastText_corpus.str.split().apply(unique_words_tok_res.update)\n",
    "unique_words_lem = lem_fastText_corpus.str.split().apply(unique_words_lem_res.update)\n",
    "print('unique words tokenized', len(unique_words_tok_res))\n",
    "print('unique words lemmatized', len(unique_words_lem_res))\n",
    "\n",
    "print('total tokenized words:', total_tok_corpus)\n",
    "print('total lemmatized words:', total_lem_corpus)\n",
    "# lem_fastText_corpus = pd.read_csv(dp.lemmatized_unique_sentences_corpus)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66a74710f051586426ab976e23d5ff4f9c170f849e5f5a0fe4458d432aa964cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('sbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
