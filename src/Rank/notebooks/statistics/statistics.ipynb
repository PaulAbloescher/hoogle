{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always reload modules to have the current version\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranking.util import json_lines as jl\n",
    "from ranking.util import dataset_paths as dp\n",
    "from ranking.normalization import normalizer as n\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144897\n"
     ]
    }
   ],
   "source": [
    "raw_df = jl.read_jsonl(dp.raw_corpus)\n",
    "raw_df = raw_df[raw_df['docItem'] != '']  # ignore all items that are no functions\n",
    "unique_types_group = raw_df.groupby('docType')\n",
    "unique_types_count = unique_types_group.ngroups\n",
    "print(unique_types_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_functions = jl.read_jsonl(dp.unique_functions_corpus)\n",
    "unique_functions_group = unique_functions.groupby('storageId')\n",
    "unique_functions_count = unique_functions_group.ngroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tokenized     lemmatized\n",
      "count  215654.000000  215654.000000\n",
      "mean       19.350742      11.123601\n",
      "std        38.030297      21.870450\n",
      "min         0.000000       0.000000\n",
      "25%         6.000000       4.000000\n",
      "50%        11.000000       6.000000\n",
      "75%        20.000000      12.000000\n",
      "max      3160.000000    1936.000000\n",
      "total tokenized words: 4173065\n",
      "total lemmatized words: 2398849\n"
     ]
    }
   ],
   "source": [
    "tok_unique_functions = jl.read_jsonl(dp.tokenized_unique_functions_corpus)\n",
    "tok_unique_functions = tok_unique_functions.groupby('storageId').first()\n",
    "\n",
    "lem_unique_functions = jl.read_jsonl(dp.lemmatized_unique_functions_corpus)\n",
    "lem_unique_functions = lem_unique_functions.groupby('storageId').first()\n",
    "\n",
    "tok_unique_functions_doc_len = tok_unique_functions['docContent'].str.split().str.len()\n",
    "lem_unique_functions_doc_len = lem_unique_functions['docContent'].str.split().str.len()\n",
    "\n",
    "# Total\n",
    "tok_unique_functions_doc_len_sum = tok_unique_functions_doc_len.sum()\n",
    "lem_unique_functions_doc_len_sum = lem_unique_functions_doc_len.sum()\n",
    "\n",
    "unique_functions_stat = pd.concat([tok_unique_functions_doc_len.describe().rename('tokenized'), lem_unique_functions_doc_len.describe().rename('lemmatized')], axis=1)\n",
    "print(unique_functions_stat)\n",
    "print('total tokenized words:', tok_unique_functions_doc_len_sum)\n",
    "print('total lemmatized words:', lem_unique_functions_doc_len_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words tokenized 114803\n",
      "unique words lemmatized 110939\n",
      "total tokenized words: 5616723\n",
      "total lemmatized words: 3259828\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tok_fastText_corpus = pd.read_csv(dp.tokenized_unique_sentences_corpus, header=None, converters={0: str}).squeeze('columns')\n",
    "lem_fastText_corpus = pd.read_csv(dp.lemmatized_unique_sentences_corpus, header=None, converters={0: str}).squeeze('columns')\n",
    "total_tok_corpus = tok_fastText_corpus.str.split().str.len().sum()\n",
    "total_lem_corpus = lem_fastText_corpus.str.split().str.len().sum()\n",
    "\n",
    "unique_words_tok_res = Counter()\n",
    "unique_words_lem_res = Counter()\n",
    "unique_words_tok = tok_fastText_corpus.str.split().apply(unique_words_tok_res.update)\n",
    "unique_words_lem = lem_fastText_corpus.str.split().apply(unique_words_lem_res.update)\n",
    "print('unique words tokenized', len(unique_words_tok_res))\n",
    "print('unique words lemmatized', len(unique_words_lem_res))\n",
    "\n",
    "print('total tokenized words:', total_tok_corpus)\n",
    "print('total lemmatized words:', total_lem_corpus)\n",
    "# lem_fastText_corpus = pd.read_csv(dp.lemmatized_unique_sentences_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query length median: 4.0\n",
      "Lemmatzied query length median: 3.0\n",
      "Tokenized query length mean: 5.234497831783503\n",
      "Lemmatzied query length mean: 4.726095074628597\n",
      "tokenized length 172492\n",
      "lemmatized length 172454\n"
     ]
    }
   ],
   "source": [
    "tok_tfidf_eval_set = jl.read_jsonl(dp.tokenized_tfidf_evaluation_set)['docQuery']\n",
    "tok_query_len_med = tok_tfidf_eval_set.str.split().str.len().median()\n",
    "tok_query_len_mean = tok_tfidf_eval_set.str.split().str.len().mean() \n",
    "\n",
    "lem_tfidf_eval_set = jl.read_jsonl(dp.lemmatized_tfidf_evaluation_set)['docQuery']\n",
    "lem_query_len_med = lem_tfidf_eval_set.str.split().str.len().median()\n",
    "lem_query_len_mean = lem_tfidf_eval_set.str.split().str.len().mean() \n",
    "\n",
    "print('Tokenized query length median:', tok_query_len_med)\n",
    "print('Lemmatzied query length median:', lem_query_len_med)\n",
    "\n",
    "print('Tokenized query length mean:', tok_query_len_mean)\n",
    "print('Lemmatzied query length mean:', lem_query_len_mean)\n",
    "\n",
    "print('tokenized length', len(tok_tfidf_eval_set.index))\n",
    "print('lemmatized length', len(lem_tfidf_eval_set.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query length median: 9.0\n",
      "Lemmatzied query length median: 5.0\n",
      "Tokenized query length mean: 9.384615384615385\n",
      "Lemmatzied query length mean: 4.846153846153846\n"
     ]
    }
   ],
   "source": [
    "unique_functions = jl.read_jsonl(dp.unique_functions_corpus).groupby('storageId').first()\n",
    "tok_manual_eval_set = jl.read_jsonl(dp.manual_evaluation_set)[['storageId','docQuery','sourceLink']]\n",
    "manual_eval_set_with_items = tok_manual_eval_set.set_index('storageId').join(unique_functions)\n",
    "tokenize_only = lambda query: n.normalize(query, stem=lambda x: x)\n",
    "pre_process = lambda query: n.normalize(query, stop=n.get_wn_stopwords())\n",
    "manual_eval_set_with_items = manual_eval_set_with_items[['docQuery', 'docItem', 'sourceLink']]\n",
    "manual_eval_set_with_items['docQueryNorm'] = manual_eval_set_with_items['docQuery'].apply(tokenize_only)\n",
    "manual_eval_set_with_items['docQueryFullyPreProc'] = manual_eval_set_with_items['docQueryNorm'].apply(pre_process)\n",
    "\n",
    "tok_man_query_len = manual_eval_set_with_items['docQueryNorm'].str.split().str.len()\n",
    "lem_man_query_len = manual_eval_set_with_items['docQueryFullyPreProc'].str.split().str.len()\n",
    "\n",
    "print('Tokenized query length median:', tok_man_query_len.median())\n",
    "print('Lemmatzied query length median:', lem_man_query_len.median())\n",
    "\n",
    "print('Tokenized query length mean:', tok_man_query_len.mean())\n",
    "print('Lemmatzied query length mean:', lem_man_query_len.mean())\n",
    "\n",
    "# print('tokenized length', len(tok_tfidf_eval_set.index))\n",
    "# print('lemmatized length', len(lem_tfidf_eval_set.index))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66a74710f051586426ab976e23d5ff4f9c170f849e5f5a0fe4458d432aa964cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('sbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
